{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "\n",
    "# this is needed to compensate for %matplotl+ib notebook's tendancy to blow up images when plotted inline\n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.autolayout'] = True\n",
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from autograd import grad\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from IPython.display import Image\n",
    "from scipy.signal import find_peaks\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from numpy.fft import *\n",
    "import matplotlib.style as style \n",
    "style.use('ggplot')\n",
    "\n",
    "# import function flattening module from autograd\n",
    "from autograd.misc.flatten import flatten_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Extract data and denoise\n",
    "\n",
    "Extract a sample set to use for training data, and denoise it with a threshold filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('../data/test.csv',delimiter = ' ')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to filter out frequencies in signal that are above the threshold\n",
    "def filter_signal(signal, threshold=5e3):\n",
    "    fourier = rfft(signal)\n",
    "    frequencies = rfftfreq(signal.size, d=20e-3/signal.size)\n",
    "    fourier[frequencies > threshold] = 0\n",
    "    return irfft(fourier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot of a clean window of data\n",
    "clean_data = data[:20000]\n",
    "\n",
    "plt.figure(figsize=(35,5))\n",
    "plt.plot(clean_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cleaned data by filtering out all frequencies greater than the threshold\n",
    "X = filter_signal(clean_data)[:4001]\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create training data\n",
    "\n",
    "The approach here is to produce input & output data for supervised learning.\n",
    "The clean signal for 5 cycles is the input data (variable X), and the output data (variable Y) is a counter for the number of cycles that starts at zero and increases linearly to one at the end of the first cycle and continues to increase linearly with the number of cycles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 5)) #init scaler, scales between 0 and 5\n",
    "Y_old = scaler.fit_transform(np.asarray([i for i in range(len(clean_data)+1)]).reshape(-1,1)) #generate supervised output data\n",
    "\n",
    "#plot the output with points for ints - represents the end of a cycle and an increment in count\n",
    "plt.figure(figsize=(35,5))\n",
    "plt.plot(Y_old)\n",
    "plt.scatter([4000,4000*2,4000*3,4000*4,4000*5],[1,2,3,4,5],color='Black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = Y_old[:4001].copy()\n",
    "plt.plot(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, c= 'red')\n",
    "plt.plot(Y, c= 'blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ùë¶(ùë°)=ùê¥sin(Œ©ùë°)cos(ùúô)+ùê¥cos(Œ©ùë°)sin(ùúô) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_new[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new = X.copy()\n",
    "# y_new = Y[1:].copy()\n",
    "y_new = Y.copy()\n",
    "\n",
    "# y_new = y_new[:,0]\n",
    "x_new = np.array([x_new])\n",
    "y_new = y_new.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_new.shape)\n",
    "print(y_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_transforms(x,w):\n",
    "#     print(\"feature transforms\")\n",
    "#     print(\"shape of w0\",w[0].shape)\n",
    "#     print(\"shape of w1\",w[1].shape)\n",
    "#     print(\"shape of x.T\",x.T.shape)\n",
    "    # calculate feature transform\n",
    "#     f = np.sin(w[0]+np.dot(x.T,w[1:]))\n",
    "#     f = w[0]*np.dot(x.T,w[1:])\n",
    "    f = w[0]*np.dot(x,w[1:])\n",
    "#     f = w[0]*f\n",
    "#     print(\"shape of f:\",f.shape)\n",
    "    return f.T\n",
    "\n",
    "def model(x,w): \n",
    "\n",
    "    # feature transformation \n",
    "    f = feature_transforms(x,w[0])\n",
    "\n",
    "    # compute linear combination and return\n",
    "    \n",
    "#     a = w[1][0] + np.dot(f.T,w[1][1:])\n",
    "    a = w[1][0] + np.dot(x,w[1][1:])\n",
    "#     a = w[1][0] + w[1][1]*f.T\n",
    "    return a.T\n",
    "\n",
    "# least squares cost\n",
    "def least_squares(w):\n",
    "    cost = np.sum((model(x_new,w) - y_new)**2)\n",
    "    #print(fusion_rule(x,w))\n",
    "    return cost/float(np.size(y_new))\n",
    "#     return cost\n",
    "\n",
    "#gradient descent optimizer\n",
    "def gradient_descent(g,alpha,max_its,w): \n",
    "    # compute gradient module using autograd\n",
    "    gradient = grad(g)\n",
    "\n",
    "    # run the gradient descent loop\n",
    "    weight_history = [w] # weight history container\n",
    "    cost_history = [g(w)] # cost function history container\n",
    "    for k in range(max_its):\n",
    "        # evaluate the gradient\n",
    "        grad_eval = gradient(w)\n",
    "\n",
    "        # take gradient descent step\n",
    "        w = w - alpha*grad_eval\n",
    "        \n",
    "        # record weight and cost\n",
    "        weight_history.append(w)\n",
    "        cost_history.append(g(w))\n",
    "    return weight_history,cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = np.array([0.1*np.random.randn(2,1),0.1*np.random.randn(2,1)])\n",
    "w = np.array(0.1*np.random.randn(2,4002,1))\n",
    "# w = np.array([0.1*np.random.randn(2,1),0.1*np.random.randn(2,1)])\n",
    "# w = np.array([0.1*np.random.randn(1),0.1*np.random.randn(2,1),0.1*np.random.randn(2,1)])\n",
    "g = least_squares;max_its = 1000;alpha_choice = 1e-5; \n",
    "weight_history_1,cost_history_1 = gradient_descent(g,alpha_choice,max_its,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.plot(range(len(cost_history_1)),cost_history_1,c='red',label='alpha = '+str(alpha_choice))\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(\"Cost Function History Plot\")\n",
    "plt.xlabel(\"Iteration Number\")\n",
    "plt.ylabel(\"Cost Function 'g(w)'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = weight_history_1[-1]\n",
    "mywave= model(x_new,w1)\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "# plt.scatter(y_new,x_new, c='blue', label = 'debt data')\n",
    "#plt.scatter(x_new,mywave,c='red',label='prediction line')\n",
    "plt.plot(y_new[0],c='red')\n",
    "plt.plot(mywave[0],c=\"green\")\n",
    "\n",
    "plt.title(\"Prediction vs actual data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_transforms(x,w):\n",
    "#     print(\"feature transforms\")\n",
    "#     print(\"shape of w0\",w[0].shape)\n",
    "#     print(\"shape of w1\",w[1].shape)\n",
    "#     print(\"shape of x.T\",x.T.shape)\n",
    "    # calculate feature transform\n",
    "#     f = np.sin(w[0]+np.dot(x.T,w[1:]))\n",
    "#     f = w[0]*np.dot(x.T,w[1:])\n",
    "    f = w[0]*np.dot(x.T,w[1:])\n",
    "#     f = w[0]*f\n",
    "#     print(\"shape of f:\",f.shape)\n",
    "    return f\n",
    "\n",
    "def model(x,w): \n",
    "\n",
    "    # feature transformation \n",
    "    f = feature_transforms(x,w[0])\n",
    "\n",
    "    # compute linear combination and return\n",
    "    \n",
    "#     a = w[1][0] + np.dot(f.T,w[1][1:])\n",
    "    a = w[1][0] + np.dot(x.T,w[1][1:])\n",
    "#     a = w[1][0] + w[1][1]*f.T\n",
    "    return a.T\n",
    "\n",
    "# least squares cost\n",
    "def least_squares(w):\n",
    "    cost = np.sum((model(x_new,w) - y_new)**2)\n",
    "    #print(fusion_rule(x,w))\n",
    "    return cost/float(np.size(y_new))\n",
    "#     return cost\n",
    "\n",
    "#gradient descent optimizer\n",
    "def gradient_descent(g,alpha,max_its,w): \n",
    "    # compute gradient module using autograd\n",
    "    gradient = grad(g)\n",
    "\n",
    "    # run the gradient descent loop\n",
    "    weight_history = [w] # weight history container\n",
    "    cost_history = [g(w)] # cost function history container\n",
    "    for k in range(max_its):\n",
    "        # evaluate the gradient\n",
    "        grad_eval = gradient(w)\n",
    "\n",
    "        # take gradient descent step\n",
    "        w = w - alpha*grad_eval\n",
    "        \n",
    "        # record weight and cost\n",
    "        weight_history.append(w)\n",
    "        cost_history.append(g(w))\n",
    "    return weight_history,cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = np.array([0.1*np.random.randn(2,1),0.1*np.random.randn(2,1)])\n",
    "# w = np.array(0.1*np.random.randn(2,4002,1))\n",
    "w = np.array([0.1*np.random.randn(2,5),0.1*np.random.randn(2,5)])\n",
    "# w = np.array([0.1*np.random.randn(1),0.1*np.random.randn(2,1),0.1*np.random.randn(2,1)])\n",
    "g = least_squares;max_its = 1500;alpha_choice = 1e-3; \n",
    "weight_history_1,cost_history_1 = gradient_descent(g,alpha_choice,max_its,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.plot(range(len(cost_history_1)),cost_history_1,c='red',label='alpha = '+str(alpha_choice))\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(\"Cost Function History Plot\")\n",
    "plt.xlabel(\"Iteration Number\")\n",
    "plt.ylabel(\"Cost Function 'g(w)'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = weight_history_1[-1]\n",
    "mywave= model(x_new,w1)\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "# plt.scatter(y_new,x_new, c='blue', label = 'debt data')\n",
    "#plt.scatter(x_new,mywave,c='red',label='prediction line')\n",
    "plt.plot(y_new[0],c='red')\n",
    "plt.plot(mywave[0],c=\"green\")\n",
    "\n",
    "plt.title(\"Prediction vs actual data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.asarray([filter_signal(clean_data)]) #equivalent to actions\n",
    "Y = np.asarray([Y_old.copy()]) #equivalent to states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple (order 1 MDP, linear) system model implementation\n",
    "def model(theta,xval):\n",
    "    #return theta[0] + theta[1]*s_t + theta[2]*a_t\n",
    "    return theta[0] + xval*theta[1]\n",
    "\n",
    "# loop for training model over all input/output action/state pairs\n",
    "def loop(theta,all_x,all_y):\n",
    "    # compute least squares over all imitator model outputs at once\n",
    "    s_predict = [all_y[:,0]]  # container for system_model state outputs\n",
    "    for t in range(all_x.shape[1]-1):\n",
    "        # get current action-state pair\n",
    "        y_t = all_y[:,t]\n",
    "        x_t = all_x[:,t]\n",
    "\n",
    "        # feed into system_model to get predicted output\n",
    "        s_hat = model(theta,x_t)\n",
    "        \n",
    "        # store prediction\n",
    "        s_predict.append(s_hat)\n",
    "        \n",
    "    # array-ify predictions and return\n",
    "    return np.array(s_predict).T\n",
    "\n",
    "\n",
    "# an implementation of the least squares cost for system identification\n",
    "# note here: s is an (1 x T) array and a an (1 x T-1) array\n",
    "def least_squares(theta,all_x,all_y):\n",
    "    # loop - runs over all action-state pairs and produces entire\n",
    "    # state prediction set\n",
    "    s_predict = loop(theta,all_x,all_y)\n",
    "\n",
    "    # compute least squares error between real and predicted states\n",
    "    cost = np.sum((s_predict[:,1:] - all_y[:,1:])**2)\n",
    "    return cost/float(all_y.shape[1]-1)\n",
    "\n",
    "# a simple initializer for this model\n",
    "def initializer():\n",
    "    return 0.1*np.random.randn(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(g,alpha,max_its,w): \n",
    "    # compute gradient module using autograd\n",
    "    gradient = grad(g)\n",
    "\n",
    "    # run the gradient descent loop\n",
    "    weight_history = [w] # weight history container\n",
    "    cost_history = [g(w,X,Y)]\n",
    "    \n",
    "    for k in range(max_its):\n",
    "        \n",
    "        # evaluate the gradient\n",
    "        grad_eval = gradient(w,X,Y)\n",
    "\n",
    "        # take gradient descent step\n",
    "        w = w - alpha*grad_eval\n",
    "        \n",
    "        # record weight and cost\n",
    "        weight_history.append(w)\n",
    "        cost_history.append(g(w,X,Y))\n",
    "        \n",
    "    return weight_history,cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=least_squares;\n",
    "w = initializer();\n",
    "k=2; alph = 1e-4;\n",
    "weight_history_1,cost_history_1 = gradient_descent(g,alph,k,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.plot(range(len(cost_history_1)),cost_history_1,c='red',label='alpha = '+str(alpha_choice))\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(\"Cost Function History Plot\")\n",
    "plt.xlabel(\"Iteration Number\")\n",
    "plt.ylabel(\"Cost Function 'g(w)'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
